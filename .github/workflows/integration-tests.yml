# Copyright (C) 2026 Moko Consulting <hello@mokoconsulting.tech>
# SPDX-License-Identifier: GPL-3.0-or-later
# FILE INFORMATION
# DEFGROUP: GitHub.Workflow
# INGROUP: MokoStandards.IntegrationTests
# REPO: https://github.com/mokoconsulting-tech/MokoStandards
# PATH: /.github/workflows/integration-tests.yml
# VERSION: 03.02.00
# BRIEF: Integration tests for all enterprise libraries with performance benchmarks
# NOTE: Tests all 10 enterprise libraries and measures performance

name: Integration Tests

on:
  schedule:
    # Run daily at 04:00 UTC
    - cron: '0 4 * * *'
  pull_request:
    branches:
      - main
    paths:
      - 'scripts/lib/**'
      - 'scripts/**/*.py'
  workflow_dispatch:
    inputs:
      library_filter:
        description: 'Filter specific library to test'
        required: false
        type: string
        default: ''
      run_benchmarks:
        description: 'Run performance benchmarks'
        required: false
        type: boolean
        default: true

permissions:
  contents: read

jobs:
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.x'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov

      - name: Create Test Directories
        run: |
          mkdir -p logs/tests
          mkdir -p logs/benchmarks
          mkdir -p logs/reports

      - name: Test Enterprise Libraries
        id: test
        run: |
          echo "## ðŸ§ª Enterprise Library Integration Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          python3 << 'EOF'
          import sys
          import json
          import time
          from pathlib import Path
          sys.path.insert(0, 'scripts/lib')
          
          test_results = {}
          failed_tests = []
          
          # List of enterprise libraries to test
          libraries = [
              'api_client',
              'audit_logger',
              'cli_framework',
              'common',
              'config_manager',
              'doc_helper',
              'enterprise_audit',
              'error_recovery',
              'metrics_collector',
              'security_validator'
          ]
          
          library_filter = '${{ github.event.inputs.library_filter }}'
          if library_filter:
              libraries = [lib for lib in libraries if library_filter in lib]
          
          print(f"Testing {len(libraries)} enterprise libraries...\n")
          
          for lib_name in libraries:
              try:
                  start_time = time.time()
                  
                  # Import and basic test
                  module = __import__(lib_name)
                  
                  # Check VERSION constant
                  version = getattr(module, 'VERSION', 'unknown')
                  
                  # Library-specific tests
                  test_passed = True
                  test_details = {'version': version}
                  
                  if lib_name == 'api_client':
                      from api_client import APIClient, CircuitState
                      client = APIClient(base_url='https://api.github.com')
                      test_details['circuit_state'] = client.circuit_state.value
                      
                  elif lib_name == 'enterprise_audit':
                      from enterprise_audit import AuditLogger
                      logger = AuditLogger(service='test', enable_file=False)
                      test_details['service'] = logger.service
                      
                  elif lib_name == 'metrics_collector':
                      from metrics_collector import MetricsCollector
                      metrics = MetricsCollector()
                      metrics.increment('test_metric')
                      test_details['test_metric'] = metrics.counters.get('test_metric', 0)
                      
                  elif lib_name == 'security_validator':
                      from security_validator import SecurityValidator
                      validator = SecurityValidator()
                      test_details['patterns'] = len(validator.CREDENTIAL_PATTERNS)
                      
                  elif lib_name == 'config_manager':
                      from config_manager import ConfigManager
                      config = ConfigManager()
                      test_details['config_loaded'] = True
                      
                  elif lib_name == 'error_recovery':
                      from error_recovery import ErrorRecovery
                      recovery = ErrorRecovery()
                      test_details['initialized'] = True
                  
                  elapsed = time.time() - start_time
                  test_details['load_time_ms'] = round(elapsed * 1000, 2)
                  
                  test_results[lib_name] = {
                      'status': 'passed',
                      'details': test_details
                  }
                  print(f"âœ… {lib_name}: PASSED ({test_details['load_time_ms']}ms)")
                  
              except Exception as e:
                  test_results[lib_name] = {
                      'status': 'failed',
                      'error': str(e)
                  }
                  failed_tests.append(lib_name)
                  print(f"âŒ {lib_name}: FAILED - {e}")
          
          # Save test results
          with open('logs/tests/integration-results.json', 'w') as f:
              json.dump(test_results, f, indent=2)
          
          # Summary
          passed = len([r for r in test_results.values() if r['status'] == 'passed'])
          total = len(test_results)
          
          with open('/tmp/test_summary.json', 'w') as f:
              json.dump({
                  'total': total,
                  'passed': passed,
                  'failed': len(failed_tests),
                  'failed_libs': failed_tests
              }, f)
          
          print(f"\n{'='*50}")
          print(f"Total: {total} | Passed: {passed} | Failed: {len(failed_tests)}")
          print(f"{'='*50}")
          
          if failed_tests:
              sys.exit(1)
          EOF
          
          # Read summary and output
          if [ -f "/tmp/test_summary.json" ]; then
            SUMMARY=$(cat /tmp/test_summary.json)
            TOTAL=$(echo $SUMMARY | python3 -c "import sys, json; print(json.load(sys.stdin)['total'])")
            PASSED=$(echo $SUMMARY | python3 -c "import sys, json; print(json.load(sys.stdin)['passed'])")
            FAILED=$(echo $SUMMARY | python3 -c "import sys, json; print(json.load(sys.stdin)['failed'])")
            
            echo "total_tests=$TOTAL" >> $GITHUB_OUTPUT
            echo "passed_tests=$PASSED" >> $GITHUB_OUTPUT
            echo "failed_tests=$FAILED" >> $GITHUB_OUTPUT
            
            echo "### Test Results" >> $GITHUB_STEP_SUMMARY
            echo "- Total libraries: **${TOTAL}**" >> $GITHUB_STEP_SUMMARY
            echo "- Passed: **${PASSED}** âœ…" >> $GITHUB_STEP_SUMMARY
            echo "- Failed: **${FAILED}** âŒ" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Performance Benchmarks
        if: github.event.inputs.run_benchmarks != 'false'
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## âš¡ Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          python3 << 'EOF'
          import sys
          import json
          import time
          from pathlib import Path
          sys.path.insert(0, 'scripts/lib')
          
          benchmarks = {}
          
          try:
              # Benchmark 1: API Client rate limiting
              from api_client import APIClient
              start = time.time()
              client = APIClient(base_url='https://api.github.com')
              for _ in range(100):
                  client._check_rate_limit()
              elapsed = time.time() - start
              benchmarks['api_client_rate_limit'] = {
                  'iterations': 100,
                  'total_time_ms': round(elapsed * 1000, 2),
                  'avg_time_ms': round(elapsed * 1000 / 100, 4)
              }
              print(f"âœ… API Client rate limit: {benchmarks['api_client_rate_limit']['avg_time_ms']}ms/op")
              
              # Benchmark 2: Audit logging
              from enterprise_audit import AuditLogger
              logger = AuditLogger(service='benchmark', enable_file=False)
              start = time.time()
              for i in range(100):
                  logger.log_event('test_event', {'iteration': i})
              elapsed = time.time() - start
              benchmarks['audit_logging'] = {
                  'iterations': 100,
                  'total_time_ms': round(elapsed * 1000, 2),
                  'avg_time_ms': round(elapsed * 1000 / 100, 4)
              }
              print(f"âœ… Audit logging: {benchmarks['audit_logging']['avg_time_ms']}ms/op")
              
              # Benchmark 3: Metrics collection
              from metrics_collector import MetricsCollector
              metrics = MetricsCollector()
              start = time.time()
              for i in range(1000):
                  metrics.increment('test_counter')
                  metrics.set_gauge('test_gauge', i)
              elapsed = time.time() - start
              benchmarks['metrics_collection'] = {
                  'iterations': 1000,
                  'total_time_ms': round(elapsed * 1000, 2),
                  'avg_time_ms': round(elapsed * 1000 / 1000, 4)
              }
              print(f"âœ… Metrics collection: {benchmarks['metrics_collection']['avg_time_ms']}ms/op")
              
              # Benchmark 4: Security scanning
              from security_validator import SecurityValidator
              validator = SecurityValidator()
              test_content = "password = 'test123'\napi_key = 'secret'\n" * 10
              start = time.time()
              for _ in range(10):
                  for pattern, _ in validator.CREDENTIAL_PATTERNS:
                      validator._scan_content(test_content, pattern, 'test.py')
              elapsed = time.time() - start
              benchmarks['security_scanning'] = {
                  'iterations': 10,
                  'total_time_ms': round(elapsed * 1000, 2),
                  'avg_time_ms': round(elapsed * 1000 / 10, 4)
              }
              print(f"âœ… Security scanning: {benchmarks['security_scanning']['avg_time_ms']}ms/op")
              
              # Save benchmarks
              with open('logs/benchmarks/performance.json', 'w') as f:
                  json.dump(benchmarks, f, indent=2)
              
              print("\nâœ… All benchmarks completed")
              
          except Exception as e:
              print(f"âš ï¸ Benchmark failed: {e}")
          EOF
          
          if [ -f "logs/benchmarks/performance.json" ]; then
            echo "âœ… Performance benchmarks completed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "See artifacts for detailed results" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Generate Integration Report
        if: always()
        run: |
          python3 << 'EOF'
          import json
          from pathlib import Path
          from datetime import datetime
          
          try:
              report = {
                  'test_date': datetime.now().isoformat(),
                  'repository': 'MokoStandards',
                  'trigger': '${{ github.event_name }}'
              }
              
              # Load test results
              test_file = Path('logs/tests/integration-results.json')
              if test_file.exists():
                  with open(test_file, 'r') as f:
                      report['tests'] = json.load(f)
              
              # Load benchmarks
              bench_file = Path('logs/benchmarks/performance.json')
              if bench_file.exists():
                  with open(bench_file, 'r') as f:
                      report['benchmarks'] = json.load(f)
              
              # Save report
              with open('logs/reports/integration-report.json', 'w') as f:
                  json.dump(report, f, indent=2)
              
              print("âœ… Integration report generated")
              
          except Exception as e:
              print(f"âš ï¸ Report generation failed: {e}")
          EOF

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: integration-test-results-${{ github.run_number }}
          path: |
            logs/tests/
            logs/benchmarks/
            logs/reports/integration-report.json
          retention-days: 30

      - name: Notify on Failure
        if: failure()
        run: |
          echo "âŒ Integration tests failed" >> $GITHUB_STEP_SUMMARY
          echo "One or more library integrations are broken" >> $GITHUB_STEP_SUMMARY
